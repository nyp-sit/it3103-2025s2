{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303a22ec",
   "metadata": {},
   "source": [
    "# Hierarchical Feature Visualization in a Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b4d9e",
   "metadata": {},
   "source": [
    "## Load a Pre-trained CNN Model.\n",
    "Load a pre-trained CNN model (e.g., VGG16, ResNet50, Inception) using a deep learning framework like Keras or PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de9217",
   "metadata": {},
   "source": [
    "The CNN Model: VGG19\n",
    "\n",
    "[VGG19 architecture](https://arxiv.org/abs/1409.1556) consists of 16 consecutive 3x3 convolution layers with max poolings in between, followed by 3 fully connected layers. We are interested in examining input patterns that excite the neurons in different convolutional layers. \n",
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/mamaj/cnn-featurevis-ece421/master/figs/vgg19_2.png\"\n",
    "     alt=\"VGG19 architecture\"\n",
    "     style=\"float: left; margin-right: 5px;\"\n",
    "     height=\"300px\" />\n",
    "<figcaption>VGG19 architecture</figcaption>\n",
    "\n",
    "</figure>\n",
    "\n",
    "We can import a VGG19 model, pretrained on [ImageNet dataset](http://www.image-net.org/), using tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Load a pre-trained VGG16 model\n",
    "model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b16366",
   "metadata": {},
   "source": [
    "## Define a Visualization Model.\n",
    "Create a new model that takes an input image and outputs the feature maps (activations) from selected intermediate layers. This allows you to inspect the features learned by each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207acf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = [layer.name for layer in model.layers if 'conv' in layer.name or 'pool' in layer.name]\n",
    "layer_outputs = [model.get_layer(name).output for name in layer_names]\n",
    "visualization_model = Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02abecf9",
   "metadata": {},
   "source": [
    "## Prepare an Input Image.\n",
    "Load and preprocess an image for which you want to visualize the features. This typically involves resizing, normalizing, and expanding dimensions to match the model's input requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b18af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img_path = 'coconut.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "# Use canonical preprocessing for VGG16 to improve stability/contrast\n",
    "img_array = preprocess_input(img_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd229d3",
   "metadata": {},
   "source": [
    "Show the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7de009",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.pyplot.imshow(img)\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e046e3",
   "metadata": {},
   "source": [
    "## Obtain Feature Maps.\n",
    "Pass the preprocessed image through the visualization_model to get the feature maps for each selected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = visualization_model.predict(img_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d1bdf",
   "metadata": {},
   "source": [
    "## Visualize Feature Maps.\n",
    "Iterate through the obtained feature maps and plot them. You can visualize individual feature maps or a selection of them to observe the patterns and activations learned at different hierarchical levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34c924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_features_per_layer = None  # set to an int to limit per-layer features\n",
    "\n",
    "for layer_name, feature_map in zip(layer_names, feature_maps):\n",
    "    if len(feature_map.shape) == 4:  # Only visualize 2D feature maps\n",
    "        total_features = feature_map.shape[-1]\n",
    "        num_features = total_features if max_features_per_layer is None else min(total_features, max_features_per_layer)\n",
    "\n",
    "        for i in range(num_features):\n",
    "            x = feature_map[0, :, :, i]\n",
    "            std = x.std()\n",
    "            if std < 1e-6:\n",
    "                x = np.zeros_like(x)\n",
    "            else:\n",
    "                x = (x - x.mean()) / std\n",
    "                x = (x * 64 + 128)\n",
    "            x = np.clip(x, 0, 255).astype('uint8')\n",
    "\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.title(f\"{layer_name} — feature {i}\")\n",
    "            plt.imshow(x, cmap='viridis')\n",
    "            plt.axis('off')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0576b",
   "metadata": {},
   "source": [
    "## Visualize Filters in the Block1_Conv1 and Block5_Conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8725318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Utility to convert a preprocessed image tensor back to displayable RGB uint8\n",
    "def deprocess_image(x: tf.Tensor) -> np.ndarray:\n",
    "    x = x[0].numpy()\n",
    "    # BGR mean values used in VGG16 preprocessing\n",
    "    x[..., 0] += 103.939\n",
    "    x[..., 1] += 116.779\n",
    "    x[..., 2] += 123.68\n",
    "    # Convert BGR to RGB\n",
    "    x = x[..., ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "# Perform gradient ascent on the input image to maximize a given filter's activation\n",
    "def visualize_filter(layer_name: str, filter_index: int, steps: int = 30, step_size: float = 1.0, img_size: int = 224) -> np.ndarray:\n",
    "    submodel = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "    # Start from random noise in pixel space, then preprocess to VGG16 space\n",
    "    init_img = tf.random.uniform((1, img_size, img_size, 3), minval=0.0, maxval=255.0)\n",
    "    img = preprocess_input(init_img)\n",
    "    img = tf.Variable(img)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(img)\n",
    "            activation = submodel(img)\n",
    "            loss = tf.reduce_mean(activation[:, :, :, filter_index])\n",
    "        grads = tape.gradient(loss, img)\n",
    "        # Normalize gradients for stable updates\n",
    "        grads = grads / (tf.math.reduce_std(grads) + 1e-8)\n",
    "        img.assign_add(step_size * grads)\n",
    "\n",
    "    return deprocess_image(img)\n",
    "\n",
    "# Visualize filters for specified layers; set max_filters_per_layer=None to run all\n",
    "layers_to_visualize = ['block1_conv1', 'block5_conv3']\n",
    "max_filters_per_layer = None  # e.g., set to 16 to limit\n",
    "\n",
    "for layer_name in layers_to_visualize:\n",
    "    num_filters = int(model.get_layer(layer_name).output.shape[-1])\n",
    "    count = num_filters if max_filters_per_layer is None else min(num_filters, max_filters_per_layer)\n",
    "    for i in range(count):\n",
    "        img = visualize_filter(layer_name, i, steps=30, step_size=1.0, img_size=224)\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.title(f\"{layer_name} — filter {i}\")\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99300f",
   "metadata": {},
   "source": [
    "plot each channel (3x3) for each filter in block5_conv3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize per-channel 3x3 kernels for each filter in block5_conv3\n",
    "# NOTE: block5_conv3 has shape (3, 3, 512, 512) => 512 filters, each with 512 input channels\n",
    "# Plotting all channels for all filters would create 262,144 tiles.\n",
    "# Use the selectors below to limit for practicality.\n",
    "\n",
    "layer_name = 'block5_conv3'\n",
    "w = model.get_layer(layer_name).get_weights()[0]  # (3, 3, Cin, Cout)\n",
    "kh, kw, cin, cout = w.shape\n",
    "print(f\"{layer_name} kernel weights shape: {w.shape} -> (kh, kw, Cin, Cout)\")\n",
    "\n",
    "# Select which filters (output channels) and input channels to display\n",
    "# Adjust these as needed. To show all channels for a single filter, set channels_to_plot = range(cin)\n",
    "filters_to_plot = list(range(0, min(2, cout)))        # e.g., first 2 filters\n",
    "channels_to_plot = list(range(0, min(16, cin)))       # e.g., first 16 input channels\n",
    "\n",
    "for f_idx in filters_to_plot:\n",
    "    num_channels = len(channels_to_plot)\n",
    "    cols = min(8, num_channels)\n",
    "    rows = int(np.ceil(num_channels / cols))\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 1.2, rows * 1.2))\n",
    "    axes = np.atleast_2d(axes)\n",
    "    fig.suptitle(f\"{layer_name} — filter {f_idx}: per-channel 3x3 kernels\", fontsize=10)\n",
    "\n",
    "    for i, c_idx in enumerate(channels_to_plot):\n",
    "        r, c = divmod(i, cols)\n",
    "        k = w[:, :, c_idx, f_idx]  # (3, 3)\n",
    "        # Normalize per-channel for visualization\n",
    "        k_min, k_max = k.min(), k.max()\n",
    "        k_norm = (k - k_min) / (k_max - k_min + 1e-8)\n",
    "        axes[r, c].imshow(k_norm, cmap='gray', vmin=0.0, vmax=1.0, interpolation='nearest')\n",
    "        axes[r, c].set_xticks([])\n",
    "        axes[r, c].set_yticks([])\n",
    "        axes[r, c].set_title(str(c_idx), fontsize=8)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(num_channels, rows * cols):\n",
    "        r, c = divmod(j, cols)\n",
    "        axes[r, c].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IT3103",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
