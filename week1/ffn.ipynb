{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3003d2c2",
   "metadata": {},
   "source": [
    "# Build a Feedforward Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc5932",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338666a8",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f7e1b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import the Fashion MNIST dataset\n",
    "train_full = datasets.FashionMNIST(root='./data', train=True, download=True)\n",
    "test_ds = datasets.FashionMNIST(root='./data', train=False, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ebae9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define split sizes and batch size\n",
    "batch_size = 10\n",
    "train_len, val_len = 50_000, 10_000\n",
    "split_generator = torch.Generator().manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0849893c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Compute normalization from training data, build datasets and loaders\n",
    "# Scale to [0,1]\n",
    "train_pixels = train_full.data.float() / 255.0\n",
    "mean = train_pixels.mean()\n",
    "std = train_pixels.std() + 1e-7\n",
    "\n",
    "# Normalize full train and test\n",
    "x_train_full = ((train_full.data.float() / 255.0) - mean) / std\n",
    "y_train_full = train_full.targets.clone()\n",
    "\n",
    "x_test = ((test_ds.data.float() / 255.0) - mean) / std\n",
    "y_test = test_ds.targets.clone()\n",
    "\n",
    "# Add channel dimension\n",
    "x_train_full = x_train_full.unsqueeze(1)\n",
    "x_test = x_test.unsqueeze(1)\n",
    "\n",
    "# Build datasets and split\n",
    "full_ds = TensorDataset(x_train_full, y_train_full)\n",
    "train_ds, val_ds = random_split(full_ds, [train_len, val_len], generator=split_generator)\n",
    "\n",
    "test_tensor_ds = TensorDataset(x_test, y_test)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_tensor_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9daede",
   "metadata": {},
   "source": [
    "## Build a Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc86909",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        # He uniform initialization without torch.empty/zeros/zeros_like\n",
    "        limit = float(np.sqrt(2.0 / in_features))\n",
    "        self.W = (torch.rand(in_features, out_features, device=device) * (2.0 * limit)) - limit\n",
    "        self.b = torch.rand(out_features, device=device)\n",
    "        self.b = self.b - self.b  # zero bias without torch.zeros\n",
    "        self.dW = self.W * 0.0    # zero grad without torch.zeros_like\n",
    "        self.db = self.b * 0.0    # zero grad without torch.zeros_like\n",
    "        self.input_cache = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.input_cache = x\n",
    "        return x @ self.W + self.b\n",
    "\n",
    "    def backward(self, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_cache\n",
    "        # Parameter gradients\n",
    "        self.dW = x.transpose(0, 1) @ grad_output\n",
    "        self.db = grad_output.sum(dim=0)\n",
    "        # Gradient w.r.t. inputs\n",
    "        grad_input = grad_output @ self.W.transpose(0, 1)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5356b75",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.mask = x > 0\n",
    "        return x.clamp_min(0.0)\n",
    "\n",
    "    def backward(self, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        return grad_output * self.mask.to(grad_output.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c7a65",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "    x_shift = x - x.max(dim=dim, keepdim=True).values\n",
    "    exp_x = torch.exp(x_shift)\n",
    "    return exp_x / exp_x.sum(dim=dim, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce8a8e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.input_shape = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.input_shape = x.shape\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "    def backward(self, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        return grad_output.view(self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c6a88",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForwardNet:\n",
    "    def __init__(self):\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = DenseLayer(28*28, 128)\n",
    "        self.relu1 = ReLU()\n",
    "        self.fc2 = DenseLayer(128, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.flatten.forward(x)\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu1.forward(x)\n",
    "        logits = self.fc2.forward(x)\n",
    "        return logits\n",
    "\n",
    "    def backward(self, grad_logits: torch.Tensor) -> None:\n",
    "        grad = self.fc2.backward(grad_logits)\n",
    "        grad = self.relu1.backward(grad)\n",
    "        grad = self.fc1.backward(grad)\n",
    "        _ = self.flatten.backward(grad)\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        self.fc1.dW *= 0.0; self.fc1.db *= 0.0\n",
    "        self.fc2.dW *= 0.0; self.fc2.db *= 0.0\n",
    "\n",
    "    def parameters(self):\n",
    "        return [(self.fc1.W, self.fc1.dW), (self.fc1.b, self.fc1.db), (self.fc2.W, self.fc2.dW), (self.fc2.b, self.fc2.db)]\n",
    "\n",
    "model = FeedForwardNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8dba1",
   "metadata": {},
   "source": [
    "## Train an Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b546f361",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits: torch.Tensor, targets: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # logits: [B, 10], targets: [B]\n",
    "    probs = softmax(logits, dim=1)\n",
    "    batch_indices = torch.arange(logits.shape[0], device=logits.device)\n",
    "    target_probs = probs[batch_indices, targets]\n",
    "    # Add epsilon for numerical stability\n",
    "    eps = 1e-12\n",
    "    losses = -torch.log(target_probs + eps)\n",
    "    return losses.mean(), probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767eaf93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, params, lr: float = 1e-3):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for weight, grad in self.params:\n",
    "            weight -= self.lr * grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for _, grad in self.params:\n",
    "            grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b8ec56",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_from_logits(logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == targets).float().mean().item() * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7cb85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = FeedForwardNet()\n",
    "optimizer = SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Train\n",
    "    model.zero_grad()\n",
    "    model_device = device\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        logits = model.forward(xb)\n",
    "        loss, probs = cross_entropy_loss(logits, yb)\n",
    "\n",
    "        # Backprop: compute gradient of loss w.r.t logits using dL/dz = probs - one_hot(target)\n",
    "        grad_logits = probs\n",
    "        grad_logits[torch.arange(yb.size(0), device=device), yb] -= 1.0\n",
    "        grad_logits /= yb.size(0)\n",
    "\n",
    "        model.zero_grad()\n",
    "        model.backward(grad_logits)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_acc += accuracy_from_logits(logits.detach(), yb)\n",
    "        total_batches += 1\n",
    "\n",
    "    train_loss = running_loss / total_batches\n",
    "    train_acc = running_acc / total_batches\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        val_loss_accum = 0.0\n",
    "        val_acc_accum = 0.0\n",
    "        val_batches = 0\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logits = model.forward(xb)\n",
    "            vloss, _ = cross_entropy_loss(logits, yb)\n",
    "            val_loss_accum += vloss.item()\n",
    "            val_acc_accum += accuracy_from_logits(logits, yb)\n",
    "            val_batches += 1\n",
    "        val_loss = val_loss_accum / val_batches\n",
    "        val_acc = val_acc_accum / val_batches\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:02d}/{num_epochs} | train_loss={train_loss:.4f} acc={train_acc:.2f}% | val_loss={val_loss:.4f} acc={val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c76eb",
   "metadata": {},
   "source": [
    "## Evaluate an Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f396b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_loss_accum = 0.0\n",
    "    test_acc_accum = 0.0\n",
    "    test_batches = 0\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        logits = model.forward(xb)\n",
    "        tloss, _ = cross_entropy_loss(logits, yb)\n",
    "        test_loss_accum += tloss.item()\n",
    "        test_acc_accum += accuracy_from_logits(logits, yb)\n",
    "        test_batches += 1\n",
    "    test_loss = test_loss_accum / test_batches\n",
    "    test_acc = test_acc_accum / test_batches\n",
    "\n",
    "print(f\"Test loss={test_loss:.4f} acc={test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
